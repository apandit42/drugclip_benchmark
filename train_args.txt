usage: unicore-train [-h] [--no-progress-bar] [--log-interval N]
                     [--log-format {json,none,simple,tqdm}]
                     [--tensorboard-logdir DIR] [--seed N] [--cpu] [--fp16]
                     [--bf16] [--bf16-sr] [--allreduce-fp32-grad]
                     [--fp16-no-flatten-grads]
                     [--fp16-init-scale FP16_INIT_SCALE]
                     [--fp16-scale-window FP16_SCALE_WINDOW]
                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                     [--min-loss-scale D]
                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE]
                     [--user-dir USER_DIR]
                     [--empty-cache-freq EMPTY_CACHE_FREQ]
                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                     [--suppress-crashes] [--profile] [--ema-decay EMA_DECAY]
                     [--validate-with-ema] [--loss {cross_entropy,masked_lm}]
                     [--optimizer {sgd,adam,adadelta,adagrad}]
                     [--lr-scheduler {inverse_sqrt,exponential_decay,reduce_lr_on_plateau,cosine,pass_through,tri_stage,fixed,triangular,polynomial_decay}]
                     [--task TASK] [--num-workers N]
                     [--skip-invalid-size-inputs-valid-test] [--batch-size N]
                     [--required-batch-size-multiple N]
                     [--data-buffer-size DATA_BUFFER_SIZE]
                     [--train-subset SPLIT] [--valid-subset SPLIT]
                     [--validate-interval N] [--validate-interval-updates N]
                     [--validate-after-updates N] [--fixed-validation-seed N]
                     [--disable-validation] [--batch-size-valid N]
                     [--max-valid-steps N] [--curriculum N]
                     [--distributed-world-size N]
                     [--distributed-rank DISTRIBUTED_RANK]
                     [--distributed-backend DISTRIBUTED_BACKEND]
                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                     [--distributed-port DISTRIBUTED_PORT]
                     [--device-id DEVICE_ID] [--distributed-no-spawn]
                     [--ddp-backend {c10d,apex,no_c10d}] [--bucket-cap-mb MB]
                     [--fix-batches-to-gpus] [--find-unused-parameters]
                     [--fast-stat-sync] [--broadcast-buffers]
                     [--nprocs-per-node NPROCS_PER_NODE] --arch ARCH
                     [--max-epoch N] [--max-update N]
                     [--stop-time-hours STOP_TIME_HOURS] [--clip-norm NORM]
                     [--per-sample-clip-norm PNORM]
                     [--update-freq N1,N2,...,N_K] [--lr LR_1,LR_2,...,LR_N]
                     [--stop-min-lr LR] [--save-dir DIR] [--tmp-save-dir DIR]
                     [--restore-file RESTORE_FILE]
                     [--finetune-from-model FINETUNE_FROM_MODEL]
                     [--load-from-ema] [--reset-dataloader]
                     [--reset-lr-scheduler] [--reset-meters]
                     [--reset-optimizer] [--optimizer-overrides DICT]
                     [--save-interval N] [--save-interval-updates N]
                     [--keep-interval-updates N] [--keep-last-epochs N]
                     [--keep-best-checkpoints N] [--no-save]
                     [--no-epoch-checkpoints] [--no-last-checkpoints]
                     [--no-save-optimizer-state]
                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
                     [--maximize-best-checkpoint-metric] [--patience N]
                     [--checkpoint-suffix CHECKPOINT_SUFFIX]

options:
  -h, --help            show this help message and exit
  --no-progress-bar     disable progress bar
  --log-interval N      log progress every N batches (when progress bar is
                        disabled)
  --log-format {json,none,simple,tqdm}
                        log format to use
  --tensorboard-logdir DIR
                        path to save logs for tensorboard, should match
                        --logdir of running tensorboard (default: no
                        tensorboard logging)
  --seed N              pseudo random number generator seed
  --cpu                 use CPU instead of CUDA
  --fp16                use FP16
  --bf16                use BF16
  --bf16-sr             use stachostic rounding for bf16
  --allreduce-fp32-grad
                        use fp32-grads in fp16/bf16 mode. --ddp-backend should
                        be no_c10d
  --fp16-no-flatten-grads
                        don't flatten FP16 grads tensor
  --fp16-init-scale FP16_INIT_SCALE
                        default FP16 loss scale
  --fp16-scale-window FP16_SCALE_WINDOW
                        number of updates before increasing loss scale
  --fp16-scale-tolerance FP16_SCALE_TOLERANCE
                        pct of updates that can overflow before decreasing the
                        loss scale
  --min-loss-scale D    minimum FP16 loss scale, after which training is
                        stopped
  --threshold-loss-scale THRESHOLD_LOSS_SCALE
                        threshold FP16 loss scale from below
  --user-dir USER_DIR   path to a python module containing custom extensions
                        (tasks and/or architectures)
  --empty-cache-freq EMPTY_CACHE_FREQ
                        how often to clear the PyTorch CUDA cache (0 to
                        disable)
  --all-gather-list-size ALL_GATHER_LIST_SIZE
                        number of bytes reserved for gathering stats from
                        workers
  --suppress-crashes    suppress crashes when training with the entry point so
                        that the main method can return a value (useful for
                        sweeps)
  --profile             enable autograd profiler emit_nvtx
  --ema-decay EMA_DECAY
                        enable moving average for model weights
  --validate-with-ema
  --loss {cross_entropy,masked_lm}
  --optimizer {sgd,adam,adadelta,adagrad}
  --lr-scheduler {inverse_sqrt,exponential_decay,reduce_lr_on_plateau,cosine,pass_through,tri_stage,fixed,triangular,polynomial_decay}
  --task TASK           task

Dataset and data loading:
  --num-workers N       how many subprocesses to use for data loading
  --skip-invalid-size-inputs-valid-test
                        ignore too long or too short lines in valid and test
                        set
  --batch-size N, --max-sentences N
                        maximum number of sentences in a batch
  --required-batch-size-multiple N
                        batch size will be a multiplier of this value
  --data-buffer-size DATA_BUFFER_SIZE
                        Number of batches to preload
  --train-subset SPLIT  data subset to use for training (train, valid, test)
  --valid-subset SPLIT  comma separated list of data subsets to use for
                        validation (train, valid, valid1, test, test1)
  --validate-interval N
                        validate every N epochs
  --validate-interval-updates N
                        validate every N updates
  --validate-after-updates N
                        dont validate until reaching this many updates
  --fixed-validation-seed N
                        specified random seed for validation
  --disable-validation  disable validation
  --batch-size-valid N  maximum number of sentences in a validation batch
                        (defaults to --max-sentences)
  --max-valid-steps N   How many batches to evaluate
  --curriculum N        don't shuffle batches for first N epochs

Distributed training:
  --distributed-world-size N
                        total number of GPUs across all nodes (default: all
                        visible GPUs)
  --distributed-rank DISTRIBUTED_RANK
                        rank of the current worker
  --distributed-backend DISTRIBUTED_BACKEND
                        distributed backend
  --distributed-init-method DISTRIBUTED_INIT_METHOD
                        typically tcp://hostname:port that will be used to
                        establish initial connetion
  --distributed-port DISTRIBUTED_PORT
                        port number (not required if using --distributed-init-
                        method)
  --device-id DEVICE_ID, --local_rank DEVICE_ID
                        which GPU to use (usually configured automatically)
  --distributed-no-spawn
                        do not spawn multiple processes even if multiple GPUs
                        are visible
  --ddp-backend {c10d,apex,no_c10d}
                        DistributedDataParallel backend
  --bucket-cap-mb MB    bucket size for reduction
  --fix-batches-to-gpus
                        don't shuffle batches between GPUs; this reduces
                        overall randomness and may affect precision but avoids
                        the cost of re-reading the data
  --find-unused-parameters
                        disable unused parameter detection (not applicable to
                        no_c10d ddp-backend
  --fast-stat-sync      Enable fast sync of stats between nodes, this
                        hardcodes to sync only some default stats from
                        logging_output.
  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as
                        batchnorm population statistics
  --nprocs-per-node NPROCS_PER_NODE
                        number of GPUs in each node. An allreduce operation
                        across GPUs in a node is very fast. Hence, we do
                        allreduce across GPUs in a node, and gossip across
                        different nodes

Model configuration:
  --arch ARCH, -a ARCH  Model Architecture

Optimization:
  --max-epoch N, --me N
                        force stop training at specified epoch
  --max-update N, --mu N
                        force stop training at specified update
  --stop-time-hours STOP_TIME_HOURS
                        force stop training after specified cumulative time
                        (if >0)
  --clip-norm NORM      clip threshold of gradients
  --per-sample-clip-norm PNORM
                        clip threshold of gradients, before gradient sync over
                        workers. In fp16/bf16 mode, --fp32-grad should be set,
                        and --dpp-backend should be no_c10d
  --update-freq N1,N2,...,N_K
                        update parameters every N_i batches, when in epoch i
  --lr LR_1,LR_2,...,LR_N, --learning-rate LR_1,LR_2,...,LR_N
                        learning rate for the first N epochs; all epochs >N
                        using LR_N (note: this may be interpreted differently
                        depending on --lr-scheduler)
  --stop-min-lr LR      stop training when the learning rate reaches this
                        minimum

Checkpointing:
  --save-dir DIR        path to save checkpoints
  --tmp-save-dir DIR    path to temporarily save checkpoints
  --restore-file RESTORE_FILE
                        filename from which to load checkpoint (default:
                        <save-dir>/checkpoint_last.pt
  --finetune-from-model FINETUNE_FROM_MODEL
                        finetune from a pretrained model; note that meters and
                        lr scheduler will be reset
  --load-from-ema       finetune from a pretrained model; note that meters and
                        lr scheduler will be reset
  --reset-dataloader    if set, does not reload dataloader state from the
                        checkpoint
  --reset-lr-scheduler  if set, does not load lr scheduler state from the
                        checkpoint
  --reset-meters        if set, does not load meters from the checkpoint
  --reset-optimizer     if set, does not load optimizer state from the
                        checkpoint
  --optimizer-overrides DICT
                        a dictionary used to override optimizer args when
                        loading a checkpoint
  --save-interval N     save a checkpoint every N epochs
  --save-interval-updates N
                        save a checkpoint (and validate) every N updates
  --keep-interval-updates N
                        keep the last N checkpoints saved with --save-
                        interval-updates
  --keep-last-epochs N  keep last N epoch checkpoints
  --keep-best-checkpoints N
                        keep best N checkpoints based on scores
  --no-save             don't save models or checkpoints
  --no-epoch-checkpoints
                        only store last and best checkpoints
  --no-last-checkpoints
                        don't store last checkpoints
  --no-save-optimizer-state
                        don't save optimizer-state as part of checkpoint
  --best-checkpoint-metric BEST_CHECKPOINT_METRIC
                        metric to use for saving "best" checkpoints
  --maximize-best-checkpoint-metric
                        select the largest metric value for saving "best"
                        checkpoints
  --patience N          early stop training if valid performance doesn't
                        improve for N consecutive validation runs; note that
                        this is influenced by --validate-interval
  --checkpoint-suffix CHECKPOINT_SUFFIX
                        suffix to add to the checkpoint file name
